{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0db238-8a5c-4de1-a91d-1440b6bc732e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, from_unixtime, date_trunc, avg, stddev_pop, percentile_approx, row_number, expr, max as spark_max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "import dlt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54011bf6-8d0b-4a55-8e5a-3b0abd298f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import table configs: table_list, schema_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd6bd81d-9513-4fad-bdd7-97a192152ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# result = dbutils.notebook.run(\"./confluent_stream_test_configs\", 60)\n",
    "# table_list, schema_list = eval(result)\n",
    "\n",
    "# Read from a YAML file\n",
    "with open('./confluent_config.yaml', 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "\n",
    "table_dict = data['table_setup']\n",
    "confluentBootstrapServers = data['confluent_setup']['server']\n",
    "confluentApiKey = data['confluent_setup']['api_key']\n",
    "confluentSecret = data['confluent_setup']['secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10afec94-0cd0-477a-b1b7-9ded0788dae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# confluentBootstrapServers = \"pkc-wzjyjg.us-east-2.aws.confluent.cloud:9092\"\n",
    "\n",
    "# # NOTE: You have the right idea here to use secrets\n",
    "# # confluentApiKey = dbutils.secrets.get(scope = \"confluentTest\", key = \"api-key\")\n",
    "# # confluentSecret = dbutils.secrets.get(scope = \"confluentTest\", key = \"secret\")\n",
    "# confluentApiKey = 'G6D3CLLKS6Y3AD5E'\n",
    "# confluentSecret = '2q98GIqZQbrYpZKLwJ97yspZT9XjxmrStlgpluMX03FSSDDMThXEuI6KzuZUkZVE'\n",
    "\n",
    "# # confluentTopicName = \"logit-equipment-json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b049b32-6b82-467b-a70f-9c0f2c7c5801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema Definitions\n",
    "\n",
    "# sensor_bronze_schema = \"\"\"\n",
    "# timestamp LONG,\n",
    "# sensor_E DOUBLE,\n",
    "# sensor_C DOUBLE,\n",
    "# sensor_B DOUBLE,\n",
    "# sensor_A DOUBLE,\n",
    "# sensor_F DOUBLE,\n",
    "# sensor_D DOUBLE,\n",
    "# energy DOUBLE,\n",
    "# turbine_id STRING,\n",
    "# UPDATE_TS BIGINT\n",
    "# \"\"\"\n",
    "\n",
    "# turbine_schema = \"\"\"\n",
    "# country STRING,\n",
    "# lat STRING,\n",
    "# location STRING,\n",
    "# long STRING,\n",
    "# model STRING,\n",
    "# state STRING,\n",
    "# turbine_id STRING,\n",
    "# UPDATE_TS BIGINT\n",
    "# \"\"\"\n",
    "\n",
    "# ship_meta_schema = \"\"\"\n",
    "# homeport STRING,\n",
    "# lat STRING,\n",
    "# long STRING,\n",
    "# model STRING,\n",
    "# ship STRING,\n",
    "# turbine_id STRING,\n",
    "# UPDATE_TS BIGINT\n",
    "# \"\"\"\n",
    "\n",
    "# historical_turbine_status_schema = \"\"\"\n",
    "# abnormal_sensor STRING,\n",
    "# end_time LONG,\n",
    "# start_time LONG,\n",
    "# turbine_id STRING,\n",
    "# UPDATE_TS BIGINT\n",
    "# \"\"\"\n",
    "\n",
    "# parts_schema = \"\"\"\n",
    "# NSN STRING,\n",
    "# height LONG,\n",
    "# production_time LONG,\n",
    "# sensors ARRAY<STRING>,\n",
    "# stock_available LONG,\n",
    "# stock_location STRING,\n",
    "# type STRING,\n",
    "# weight LONG,\n",
    "# width LONG,\n",
    "# UPDATE_TS BIGINT\n",
    "# \"\"\"\n",
    "\n",
    "# # Dictionary of table names and info, info containes schema, confluent topic, and comment\n",
    "# table_dict = {\n",
    "#     # 'sensor_bronze': {\n",
    "#     #   'schema': sensor_bronze_schema,\n",
    "#     #   'confluentTopic': 'ADD TOPIC',\n",
    "#     #   'comment': 'ADD COMMENT'\n",
    "#     #   },\n",
    "#     'parts': {\n",
    "#       'schema': parts_schema,\n",
    "#       'confluentTopic': 'parts',\n",
    "#       'comment': 'Turbine parts from our manufacturing system',\n",
    "#       'expectation': 'ADD EXPECTATION'\n",
    "#       },\n",
    "#     'ship_meta': {\n",
    "#       'schema': ship_meta_schema,\n",
    "#       'confluentTopic': 'ship-meta',\n",
    "#       'comment': 'hip to turbine Meta_Data mapping'\n",
    "#       },\n",
    "#     'historical_turbine_status': {\n",
    "#       'schema': historical_turbine_status_schema,\n",
    "#       'confluentTopic': 'historical-turbine-status',\n",
    "#       'comment': 'Turbine status to be used as label in our predictive maintenance model (to know which turbine is potentially faulty)'\n",
    "#       },\n",
    "#     'turbine': {\n",
    "#       'schema': turbine_schema,\n",
    "#       'confluentTopic': 'turbine',\n",
    "#       'comment': 'Turbine details, with location, wind turbine model type etc'\n",
    "#       }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65aab596-534c-46b4-9fef-38dd47fc4539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def stream_and_parse_confluent(table, info):\n",
    "  bronze_name = f\"{table}_bronze\"\n",
    "  silver_name = f\"{table}_silver\"\n",
    "\n",
    "  @dlt.table(name=bronze_name, comment=f\"Raw streamed data from Confuent for table {table}\")\n",
    "  def confluent_stream(topic=info['confluentTopic']):\n",
    "    return (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"subscribe\", topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .load()\n",
    "    .withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "    )\n",
    "    \n",
    "  @dlt.table(name=silver_name, comment=info['comment'])  \n",
    "  def confluent_stream_parsed(bronze_name=bronze_name, schema=info['schema']):\n",
    "    return (\n",
    "      spark\n",
    "      .read\n",
    "      .table(bronze_name)\n",
    "      .withColumn(\"value_parsed\", from_json(\"value\", schema))\n",
    "      .select(col(\"value_parsed.*\"))\n",
    "      .drop('UPDATE_TS')\n",
    "    )\n",
    "\n",
    "for table, info in table_dict.items():\n",
    "  stream_and_parse_confluent(table, info)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e6671e5-2a2c-46a9-ad2d-be4f144d0d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = spark.conf.get(\"catalog\")\n",
    "db = spark.conf.get(\"db\")\n",
    "\n",
    "@dlt.table(\n",
    "  comment=\"Raw sensor data coming from json files ingested in incremental with Auto Loader: vibration, energy produced etc. 1 point every X sec per sensor.\",\n",
    "  table_properties={\n",
    "    \"quality\": \"bronze\"\n",
    "  }\n",
    ")\n",
    "@dlt.expect_or_drop(\"correct_schema\", \"_rescued_data IS NULL\")\n",
    "@dlt.expect_or_drop(\"correct_energy\", \"energy IS NOT NULL and energy > 0\")\n",
    "def sensor_bronze():\n",
    "  return (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"parquet\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(f\"/Volumes/{catalog}/{db}/navy_raw_landing/incoming_data\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d42a1eab-193d-4344-a54b-c7a8b57df6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  comment=\"Hourly sensor stats, used to describe signal and detect anomalies\",\n",
    "  table_properties={\n",
    "    \"quality\": \"silver\"\n",
    "  }\n",
    ")\n",
    "@dlt.expect_or_drop(\"turbine_id_valid\", \"turbine_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"timestamp_valid\", \"hourly_timestamp IS NOT NULL\")\n",
    "def sensor_hourly():\n",
    "  return (\n",
    "    dlt.read(\"sensor_bronze\")\n",
    "      .withColumn(\"hourly_timestamp\", date_trunc('hour', from_unixtime(col(\"timestamp\"))))\n",
    "      .groupBy(\"hourly_timestamp\", \"turbine_id\")\n",
    "      .agg(\n",
    "        avg(\"energy\").alias(\"avg_energy\"),\n",
    "        stddev_pop(\"sensor_A\").alias(\"std_sensor_A\"),\n",
    "        stddev_pop(\"sensor_B\").alias(\"std_sensor_B\"),\n",
    "        stddev_pop(\"sensor_C\").alias(\"std_sensor_C\"),\n",
    "        stddev_pop(\"sensor_D\").alias(\"std_sensor_D\"),\n",
    "        stddev_pop(\"sensor_E\").alias(\"std_sensor_E\"),\n",
    "        stddev_pop(\"sensor_F\").alias(\"std_sensor_F\"),\n",
    "        percentile_approx(\"sensor_A\", [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_sensor_A\"),\n",
    "        percentile_approx(\"sensor_B\", [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_sensor_B\"),\n",
    "        percentile_approx(\"sensor_C\", [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_sensor_C\"),\n",
    "        percentile_approx(\"sensor_D\", [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_sensor_D\"),\n",
    "        percentile_approx(\"sensor_E\", [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_sensor_E\"),\n",
    "        percentile_approx(\"sensor_F\", [0.1, 0.3, 0.6, 0.8, 0.95]).alias(\"percentiles_sensor_F\")\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ee82191-6e36-446f-a8c1-e071ec5e868f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  comment=\"Hourly sensor stats, used to describe signal and detect anomalies\"\n",
    ")\n",
    "def turbine_training_dataset():\n",
    "    sensor_hourly = dlt.read(\"sensor_hourly\")\n",
    "    turbine = dlt.read(\"turbine_silver\")\n",
    "    historical_turbine_status = dlt.read(\"historical_turbine_status_silver\")\n",
    "    \n",
    "    return (\n",
    "        sensor_hourly.alias(\"m\")\n",
    "        .join(turbine.alias(\"t\"), \"turbine_id\")\n",
    "        .join(\n",
    "            historical_turbine_status.alias(\"s\"),\n",
    "            (sensor_hourly.turbine_id == historical_turbine_status.turbine_id) &\n",
    "            (from_unixtime(historical_turbine_status.start_time) < sensor_hourly.hourly_timestamp) &\n",
    "            (from_unixtime(historical_turbine_status.end_time) > sensor_hourly.hourly_timestamp)\n",
    "        )\n",
    "        .selectExpr(\"* except(m.turbine_id)\")\n",
    "    )\n",
    "    #Removed t._rescued_data, s._rescued_data since stream refactor does not contain expectations\n",
    "    # Original line .selectExpr(\"* except(t._rescued_data, s._rescued_data, m.turbine_id)\")\n",
    "    # ADD BACK IN IF USING EXPECTATIONS IN STREAM INGEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fadf5a91-185b-430e-b4e0-0eea84d2698b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  comment=\"Navy gas turbine last status based on model prediction\"\n",
    ")\n",
    "def turbine_current_status():\n",
    "    latest_metrics = (\n",
    "        dlt.read(\"sensor_hourly\")\n",
    "        .join(dlt.read(\"turbine_silver\"), on=\"turbine_id\")\n",
    "        .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"turbine_id\", \"hourly_timestamp\").orderBy(col(\"hourly_timestamp\").desc())))\n",
    "    )\n",
    "    \n",
    "    return latest_metrics.filter(\"row_number = 1\").selectExpr(\n",
    "        \"* EXCEPT(row_number)\", \n",
    "        \"predict_maintenance(hourly_timestamp, avg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F, percentiles_sensor_A, percentiles_sensor_B, percentiles_sensor_C, percentiles_sensor_D, percentiles_sensor_E, percentiles_sensor_F, location, model, state) as prediction\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eec16e2-189b-4b78-b6e6-deb8faabc3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table\n",
    "def fleet_current_status_gold():\n",
    "    turbine_current_status = (\n",
    "      dlt.read(\"turbine_current_status\")\n",
    "      .select(\n",
    "        \"turbine_id\",\n",
    "        \"hourly_timestamp\",\n",
    "        \"prediction\"\n",
    "        )\n",
    "      )\n",
    "    \n",
    "    ship_meta = dlt.read(\"ship_meta_silver\")\n",
    "    sensor_maintenance = spark.table(f\"{catalog}.{db}.sensor_maintenance\")\n",
    "    parts = dlt.read(\"parts_silver\")\n",
    "    max_hourly_timestamp = turbine_current_status.agg(spark_max(\"hourly_timestamp\")).collect()[0][0]\n",
    "    \n",
    "    return (\n",
    "      turbine_current_status\n",
    "      .join(ship_meta, \"turbine_id\")\n",
    "      .join(sensor_maintenance, turbine_current_status.prediction == sensor_maintenance.fault, \"left\")\n",
    "      .filter(col(\"hourly_timestamp\") == max_hourly_timestamp)\n",
    "      .select(\n",
    "        *[col(f\"{c}\") for c in turbine_current_status.columns],\n",
    "        *[col(f\"{c}\") for c in ship_meta.columns if c != \"turbine_id\"],\n",
    "        *[col(f\"{c}\") for c in sensor_maintenance.columns]\n",
    "   )\n",
    ")\n",
    "    #     turbine_current_status.alias(\"t\")\n",
    "    #     .join(ship_meta.alias('s'), \"turbine_id\")\n",
    "    #     .join(sensor_maintenance.alias('m'), turbine_current_status.prediction == sensor_maintenance.fault, \"left\")\n",
    "    #     .join(parts.alias(\"p\"), expr(\"array_contains(p.sensors, t.prediction)\"), \"left\")\n",
    "    #     .filter(col(\"hourly_timestamp\") == max_hourly_timestamp)\n",
    "    #     .selectExpr(\"* EXCEPT(s.turbine_id)\")\n",
    "    # )\n",
    "\n",
    "# (turbine_current_status\n",
    "#  .join(ship_meta, \"turbine_id\")\n",
    "#  .join(sensor_maintenance, turbine_current_status.prediction == sensor_maintenance.fault, \"left\")\n",
    "#  .filter(col(\"hourly_timestamp\") == max_hourly_timestamp)\n",
    "#  .select(\n",
    "#    *[col(f\"{c}\") for c in ship_meta.columns if c != \"turbine_id\"],\n",
    "#    *[col(f\"{c}\") for c in sensor_maintenance.columns]\n",
    "#    )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1315887243012124,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "confluent_connector_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
