resources:
  jobs:
    pdm_job:
      name: "pdm_${var.demo_type}_job"
      tasks:
        - task_key: init_data
          notebook_task:
            notebook_path: ../src/_resources/01-load-data.py
          job_cluster_key: "${var.demo_type}_cluster"
        - task_key: start_dlt_pipeline
          depends_on:
            - task_key: init_data
          pipeline_task:
            pipeline_id: ${resources.pipelines.pdm_pipeline.id}
            full_refresh: true
        - task_key: load_dbsql_and_ml_data
          depends_on:
            - task_key: start_dlt_pipeline
          notebook_task:
            notebook_path: ../src/_resources/00-prep-data-db-sql.py
            base_parameters:
              reset_all_data: "false"
        - task_key: create_feature_and_automl_run
          depends_on:
            - task_key: load_dbsql_and_ml_data
          notebook_task:
            notebook_path: ../src/04-Data-Science-ML/04.1-automl-iot-turbine-predictive-maintenance.py
          job_cluster_key: "${var.demo_type}_cluster"
        - task_key: register_ml_model
          depends_on:
            - task_key: create_feature_and_automl_run
          notebook_task:
            notebook_path: ../src/04-Data-Science-ML/04.2-AutoML-best-register-model.ipynb
          job_cluster_key: "${var.demo_type}_cluster"
        - task_key: optimize_supply_routing
          depends_on:
            - task_key: register_ml_model
          notebook_task:
            notebook_path: ../src/05-Supply-Optimization/05.1_Optimize_Transportation.py
          job_cluster_key: "${var.demo_type}_cluster"
        - task_key: AI_Work_Parts_Orders
          depends_on:
            - task_key: optimize_supply_routing
          notebook_task:
            notebook_path: ../src/07-Generate-tickets/AI_work_orders.ipynb
          job_cluster_key: "${var.demo_type}_cluster"
      job_clusters:
        - job_cluster_key: "${var.demo_type}_cluster"
          new_cluster:
            spark_version: 15.1.x-cpu-ml-scala2.12
            spark_conf:
              spark.master: local[*, 4]
              spark.databricks.cluster.profile: singleNode
              spark.databricks.dataLineage.enabled: "true"
            custom_tags:
              ResourceClass: SingleNode
              project: dbdemos
              demo: "lakehouse-${var.demo_type}-maintenance"
            spark_env_vars:
              PYSPARK_PYTHON: /databricks/python3/bin/python3
            instance_pool_id: ${var.instance_pool_id}
            data_security_mode: SINGLE_USER
            runtime_engine: STANDARD
            num_workers: 0
      queue:
        enabled: true
