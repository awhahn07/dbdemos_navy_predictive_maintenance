{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0db238-8a5c-4de1-a91d-1440b6bc732e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e152080-0e12-4f9d-b188-0b7c89bdf57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = dbutils.notebook.run(\"./confluent_stream_test_configs\", 60)\n",
    "table_list, schema_list = eval(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10afec94-0cd0-477a-b1b7-9ded0788dae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "confluentBootstrapServers = \"pkc-wzjyjg.us-east-2.aws.confluent.cloud:9092\"\n",
    "\n",
    "# NOTE: You have the right idea here to use secrets\n",
    "# confluentApiKey = dbutils.secrets.get(scope = \"confluentTest\", key = \"api-key\")\n",
    "# confluentSecret = dbutils.secrets.get(scope = \"confluentTest\", key = \"secret\")\n",
    "confluentApiKey = 'G6D3CLLKS6Y3AD5E'\n",
    "confluentSecret = '2q98GIqZQbrYpZKLwJ97yspZT9XjxmrStlgpluMX03FSSDDMThXEuI6KzuZUkZVE'\n",
    "\n",
    "confluentTopicName = \"parts\"\n",
    "\n",
    "bronze_stream_checkpoint = f\"/tmp/ahahn/checkpoints/{confluentTopicName}_bronze\"\n",
    "bronze_table_name = f\"ahahn_demo.confluent_navy_test.`{confluentTopicName}_bronze`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c0e5ca-03b6-4c82-91c5-e7090163b687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reset_stream(checkpoint_location: str, table_name: str):\n",
    "  assert checkpoint_location.startswith(\"/tmp/\"), \"Checkpoint location has to be in /tmp/\"\n",
    "\n",
    "  print(\"Deleting checkpoint\")\n",
    "  dbutils.fs.rm(checkpoint_location, True)\n",
    "\n",
    "  print(\"Dropping table\")\n",
    "  spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "reset_stream(bronze_stream_checkpoint, bronze_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c43aed-b74f-4cb0-9051-798138474ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_streaming = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret))\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .load()\n",
    "    .withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7450837d-da0b-4ee5-abeb-5fb8488bd98c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_streaming\n",
    "  .writeStream\n",
    "  .queryName(\"confluent_stream_query\")\n",
    "  .option(\"checkpointLocation\", bronze_stream_checkpoint)\n",
    "  .toTable(bronze_table_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9febc355-92b3-4dd9-8869-9b6a39eea422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select * from ahahn_demo.confluent_navy_test.`historical-turbine-status_bronze`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d09ac6f-7079-4966-b304-cd3920304e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- GRANT ALL PRIVILEGES ON CATALOG ahahn_demo TO `alex.vanadio@databricks.com`;\n",
    "-- TODO: Look up the revoke command. I don't recall what it is\n",
    "-- REVOKE ALL PRIVILEGES ON CATALOG ahahn_demo TO `alex.vanadio@databricks.com`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ab3d2d5-7781-4666-bf74-34b7735856a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# json_schema = StructType([\n",
    "#     StructField(\"EQUIPMENTID\", StringType(), True),\n",
    "#     StructField(\"WORKCENTERID\", IntegerType(), True),\n",
    "#     StructField(\"WORKCENTERNAME\", StringType(), True),\n",
    "#     StructField(\"EQUIPMENTTITLE\", StringType(), True),\n",
    "#     StructField(\"EQUIPMENTLOCATION\", StringType(), True),\n",
    "#     StructField(\"EQUIPMENTSERIALNUMBER\", StringType(), True),\n",
    "#     StructField(\"EQUIPMENTNOTE\", StringType(), True),\n",
    "#     StructField(\"EQUIPMENTGROUP\", StringType(), True),\n",
    "#     StructField(\"PRIMARYMIP\", StringType(), True),\n",
    "#     StructField(\"PRIMARYMIPBDAY\", StringType(), True),\n",
    "#     StructField(\"CDMRIN\", StringType(), True),\n",
    "#     StructField(\"DAMAGECONTROL\", BooleanType(), True),\n",
    "#     StructField(\"UPDATE_TS\", LongType(), True)\n",
    "# ])\n",
    "\n",
    "# NOTE: THIS is a less painful way of defining schemas\n",
    "json_schema = \"\"\"\n",
    "NSN STRING,\n",
    "height LONG,\n",
    "production_time LONG,\n",
    "sensors ARRAY<STRING>,\n",
    "stock_available LONG,\n",
    "stock_location STRING,\n",
    "type STRING,\n",
    "weight LONG,\n",
    "width LONG,\n",
    "UPDATE_TS BIGINT\n",
    "\"\"\"\n",
    "\n",
    "(spark.table(bronze_table_name)\n",
    "    .withColumn(\"value_parsed\", from_json(\"value\", json_schema))\n",
    "    .select(col(\"value_parsed.*\"))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a586aa-03eb-4a46-b2ca-8d2fbe04aabb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When validating, _rescued_data may not exists. Also, nested array in parts table schema definition might break\n",
    "\n",
    "sensor_bronze_schema = \"\"\"\n",
    "timestamp LONG,\n",
    "sensor_E DOUBLE,\n",
    "sensor_C DOUBLE,\n",
    "sensor_B DOUBLE,\n",
    "sensor_A DOUBLE,\n",
    "sensor_F DOUBLE,\n",
    "sensor_D DOUBLE,\n",
    "energy DOUBLE,\n",
    "turbine_id STRING,\n",
    "\"\"\"\n",
    "\n",
    "turbine_schema = \"\"\"\n",
    "country STRING,\n",
    "lat STRING,\n",
    "location STRING,\n",
    "long STRING,\n",
    "model STRING,\n",
    "state STRING,\n",
    "turbine_id STRING,\n",
    "\"\"\"\n",
    "\n",
    "ship_meta_schema = \"\"\"\n",
    "homeport STRING,\n",
    "lat STRING,\n",
    "long STRING,\n",
    "model STRING,\n",
    "ship STRING,\n",
    "turbine_id STRING,\n",
    "\"\"\"\n",
    "\n",
    "historical_turbine_status_schema = \"\"\"\n",
    "abnormal_sensor STRING,\n",
    "end_time LONG,\n",
    "start_time LONG,\n",
    "turbine_id STRING,\n",
    "\"\"\"\n",
    "\n",
    "parts_schema = \"\"\"\n",
    "NSN STRING,\n",
    "height LONG,\n",
    "production_time LONG,\n",
    "sensors ARRAY<STRING>,\n",
    "stock_available LONG,\n",
    "stock_location STRING,\n",
    "type STRING,\n",
    "weight LONG,\n",
    "width LONG,\n",
    "\"\"\"\n",
    "\n",
    "table_list = ['sensor_bronze', 'parts', 'ship_meta', 'historical_turbine_status', 'turbine']\n",
    "\n",
    "table_dict = {\n",
    "    # 'sensor_bronze': {'schema': sensor_bronze_schema, 'confluentTopic': 'ADD TOPIC', 'comment': 'ADD COMMENT'},\n",
    "    'parts': {'schema': parts_schema, 'confluentTopic': 'parts', 'comment': 'ADD COMMENT'},\n",
    "    # 'ship_meta': {'schema': ship_meta_schema, 'confluentTopic': 'ship-meta', 'comment': 'ADD COMMENT'},\n",
    "    # 'historical_turbine_status': {'schema': historical_turbine_status_schema, 'confluentTopic': 'historical-turbine-status', 'comment': 'ADD COMMENT'},\n",
    "    # 'turbine': {'schema': turbine_schema, 'confluentTopic': 'turbine', 'comment': 'ADD COMMENT'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a215453-7e31-4d22-bba9-02892417ce23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table, info in table_dict.items():\n",
    "  print(info['schema'])\n",
    "  print(parts_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d57ad9-3af9-4cc3-b9b7-e421a54ac8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "latest_metrics = (\n",
    "    spark.table(\"ahahn_demo.dbdemos_navy_pdm.sensor_hourly\")\n",
    "    .join(spark.table(\"ahahn_demo.dbdemos_navy_pdm.turbine\"), on=\"turbine_id\")\n",
    "    .withColumn(\"row_number\", row_number().over(Window.partitionBy(\"turbine_id\", \"hourly_timestamp\").orderBy(col(\"hourly_timestamp\").desc())))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a1c487-594b-4118-8515-46b3c357c968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "turbine_current_status = ( spark.table(\"ahahn_demo.dbdemos_navy_pdm.turbine_current_status\")\n",
    "                          .select(\n",
    "                            \"turbine_id\",\n",
    "                            \"hourly_timestamp\",\n",
    "                            \"prediction\"\n",
    "                            )\n",
    "                          )\n",
    "ship_meta = spark.table(\"ahahn_demo.dbdemos_navy_pdm.ship_meta\")\n",
    "sensor_maintenance = spark.table(\"ahahn_demo.dbdemos_navy_pdm.sensor_maintenance\")\n",
    "\n",
    "max_hourly_timestamp = turbine_current_status.agg(spark_max(\"hourly_timestamp\")).collect()[0][0]\n",
    "\n",
    "\n",
    "#     turbine_current_status\n",
    "#     .join(ship_meta, \"turbine_id\")\n",
    "#     .join(sensor_maintenance, turbine_current_status.prediction == sensor_maintenance.fault, \"left\")\n",
    "#     .filter(col(\"hourly_timestamp\") == max_hourly_timestamp)\n",
    "#     .select(\n",
    "#         turbine_current_status.turbine_id,\n",
    "#         turbine_current_status.hourly_timestamp,\n",
    "#         turbine_current_status.prediction,\n",
    "#         *[col(f\"s.{c}\") for c in ship_meta.columns if c != \"turbine_id\"],\n",
    "#         *[col(f\"m.{c}\") for c in sensor_maintenance.columns]\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5693ff0-e85f-41df-a7ca-fb4e280a17b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = ( turbine_current_status\n",
    "      .join(ship_meta, \"turbine_id\")\n",
    "      .join(sensor_maintenance, turbine_current_status.prediction == sensor_maintenance.fault, \"left\")\n",
    "      .filter(col(\"hourly_timestamp\") == max_hourly_timestamp)\n",
    "      .select(\n",
    "        *[col(f\"{c}\") for c in ship_meta.columns if c != \"turbine_id\"],\n",
    "        *[col(f\"{c}\") for c in sensor_maintenance.columns]\n",
    "    )\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b433a3c-2c71-4f1f-af45-19552b5e7af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "ship_status_df = spark.table(\"ahahn_demo.dbdemos_navy_pdm.ship_current_status_gold\")\n",
    "parts_df = spark.table(\"ahahn_demo.dbdemos_navy_pdm.parts\")\n",
    "\n",
    "result_df = (ship_status_df.alias(\"s\") \n",
    "    .join(parts_df.alias(\"p\"), F.expr(\"array_contains(p.sensors, s.prediction)\"), \"left\") )\n",
    "\n",
    "result_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1315887243012124,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "confluent_connector",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
