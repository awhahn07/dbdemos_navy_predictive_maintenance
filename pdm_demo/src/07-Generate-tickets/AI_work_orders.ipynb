{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8995a00b-cd0e-465f-b2f8-84b76cb3a58a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5018f2e9-2e56-40e9-b622-6c87a0e3edee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f'USE CATALOG {catalog}')\n",
    "spark.sql(f'USE SCHEMA {db}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7e720b-9db9-4f12-97bd-c56b03f42932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import row_number, concat, lpad, col, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "shipment_recommendations = spark.table('shipment_recommendations')\n",
    "\n",
    "filtered = shipment_recommendations.filter(col(\"qty_shipped\") > 0)\n",
    "unique_pairs = (\n",
    "    filtered.select(\"stock_location_id\", \"designator_id\")\n",
    "    .distinct()\n",
    "    .withColumn(\n",
    "        \"rn\",\n",
    "        row_number().over(Window.orderBy(\"stock_location_id\", \"designator_id\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "parts_orders = (\n",
    "    filtered.join(\n",
    "        unique_pairs,\n",
    "        on=[\"stock_location_id\", \"designator_id\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"order_number\",\n",
    "        concat(\n",
    "            lit(\"PR-\"),\n",
    "            lpad(col(\"rn\").cast(\"string\"), 6, \"0\")\n",
    "        )\n",
    "    )\n",
    "    .drop('rn')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae78a14-a934-4223-af0a-4548131c11a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parts_orders.write.mode(\"overwrite\").saveAsTable(\"ai_part_orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f88197-a842-4f29-83e4-1c7793e67e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status = spark.table('ship_current_status_gold')\n",
    "parts = spark.table('parts_silver').select('NSN', 'type', 'sensors').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81167270-e30a-4527-8d91-3c3d4f75ee4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = \"\"\"\n",
    "WITH parts_ex AS (\n",
    "  SELECT type, EXPLODE(sensors) AS sensor\n",
    "  FROM parts_meta\n",
    ")\n",
    "SELECT\n",
    "  scsg.turbine_id,\n",
    "  scsg.prediction,\n",
    "  COLLECT_LIST(parts_ex.type) AS parts_required\n",
    "FROM ship_current_status_gold scsg\n",
    "JOIN parts_ex\n",
    "  ON scsg.prediction = parts_ex.sensor\n",
    "WHERE scsg.prediction != 'ok'\n",
    "GROUP BY scsg.turbine_id, scsg.prediction\n",
    "\"\"\"\n",
    "df = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9426c9-9ca9-469e-9f74-a2b273adf4ec",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759501952457}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, format_string, when, collect_set\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "df = df.withColumn(\n",
    "    \"work_order\",\n",
    "    format_string(\"AI-WO %03d\", row_number().over(window))\n",
    ")\n",
    "\n",
    "scsg = spark.table('ship_current_status_gold')\n",
    "\n",
    "ai_wo = scsg.join(\n",
    "    df.select(\"turbine_id\", \"prediction\", \"parts_required\", \"work_order\"),\n",
    "    on=[\"turbine_id\", \"prediction\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "ai_wo = ai_wo.withColumn(\n",
    "    \"priority\",\n",
    "    when(ai_wo.maintenance_type == \"Organizational Level\", \"Routine\")\n",
    "    .when(ai_wo.maintenance_type == \"Intermediate Level\", \"Urgent\")\n",
    "    .when(ai_wo.maintenance_type == \"Depot Level\", \"CASREP\")\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "\n",
    "# Example: group by a column and collapse another column's values into a list\n",
    "collapsed_df = parts_orders.groupBy(\"designator\").agg(collect_set(\"order_number\").alias(\"part_orders\"))\n",
    "\n",
    "ai_wo = ai_wo.join(collapsed_df, on=\"designator\", how=\"inner\")\n",
    "\n",
    "display(ai_wo)\n",
    "\n",
    "ai_wo.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"ai_work_orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a54ab303-9dd9-4c1f-a649-91a258612c31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parts orders\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "AI_work_orders",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
